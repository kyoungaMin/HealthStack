"""
PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
E-utilities APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ê²€ìƒ‰ ë° ìºì‹±
"""
import os
import sys
import hashlib
import requests
from typing import Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
from dotenv import load_dotenv

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from database.supabase_client import get_supabase_client

load_dotenv()


@dataclass
class PubMedPaper:
    """PubMed ë…¼ë¬¸ ì •ë³´"""
    pmid: str
    title: str
    abstract: str
    journal: str
    pub_year: int
    url: str
    summary_ko: Optional[str] = None


class PubMedService:
    """PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def __init__(self):
        self.api_key = os.getenv("PUBMED_API_KEY", "")
        self.db = get_supabase_client()
    
    def search_papers(
        self, 
        query: str, 
        max_results: int = 3,
        use_cache: bool = True
    ) -> list[PubMedPaper]:
        """
        PubMedì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ë¬¸)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            list[PubMedPaper]: ë…¼ë¬¸ ëª©ë¡
        """
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached = self._get_cached_papers(query)
            if cached:
                return cached[:max_results]
        
        # PubMed ê²€ìƒ‰
        pmids = self._search_pmids(query, max_results)
        if not pmids:
            return []
        
        # ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        papers = self._fetch_paper_details(pmids)
        
        # ìºì‹œ ì €ì¥
        if papers:
            self._cache_papers(query, papers)
        
        return papers
    
    def search_by_symptom_and_ingredient(
        self, 
        symptom_id: int, 
        rep_code: str
    ) -> list[PubMedPaper]:
        """
        ì¦ìƒ-ì‹ì¬ë£Œ ì¡°í•©ìœ¼ë¡œ ë…¼ë¬¸ ê²€ìƒ‰
        symptom_pubmed_map, ingredient_pubmed_map í…Œì´ë¸” í™œìš©
        """
        try:
            # ì¦ìƒ í‚¤ì›Œë“œ ì¡°íšŒ
            symptom_result = self.db.table("symptom_pubmed_map").select(
                "keyword_en, mesh_term"
            ).eq("symptom_id", symptom_id).order("priority").limit(2).execute()
            
            # ì‹ì¬ë£Œ í‚¤ì›Œë“œ ì¡°íšŒ
            ingredient_result = self.db.table("ingredient_pubmed_map").select(
                "ingredient_name_en, mesh_term, bioactive_compound"
            ).eq("rep_code", rep_code).order("priority").limit(2).execute()
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            symptom_terms = []
            for row in symptom_result.data:
                if row.get("mesh_term"):
                    symptom_terms.append(f'"{row["mesh_term"]}"[MeSH]')
                elif row.get("keyword_en"):
                    symptom_terms.append(row["keyword_en"])
            
            ingredient_terms = []
            for row in ingredient_result.data:
                if row.get("mesh_term"):
                    ingredient_terms.append(f'"{row["mesh_term"]}"[MeSH]')
                elif row.get("ingredient_name_en"):
                    ingredient_terms.append(row["ingredient_name_en"])
                if row.get("bioactive_compound"):
                    ingredient_terms.append(row["bioactive_compound"])
            
            if not symptom_terms or not ingredient_terms:
                return []
            
            # ANDë¡œ ì¡°í•©
            query = f"({' OR '.join(symptom_terms)}) AND ({' OR '.join(ingredient_terms)})"
            
            return self.search_papers(query, max_results=2)
            
        except Exception as e:
            print(f"ì¦ìƒ-ì‹ì¬ë£Œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_pmids(self, query: str, max_results: int) -> list[str]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ PMID ëª©ë¡ ì¡°íšŒ"""
        try:
            params = {
                "db": "pubmed",
                "term": query,
                "retmax": max_results,
                "retmode": "json",
                "sort": "relevance"
            }
            if self.api_key:
                params["api_key"] = self.api_key
            
            response = requests.get(
                f"{self.BASE_URL}/esearch.fcgi",
                params=params,
                timeout=10
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("esearchresult", {}).get("idlist", [])
            
        except Exception as e:
            print(f"PMID ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _fetch_paper_details(self, pmids: list[str]) -> list[PubMedPaper]:
        """PMIDë¡œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        try:
            params = {
                "db": "pubmed",
                "id": ",".join(pmids),
                "retmode": "xml"
            }
            if self.api_key:
                params["api_key"] = self.api_key
            
            response = requests.get(
                f"{self.BASE_URL}/efetch.fcgi",
                params=params,
                timeout=15
            )
            response.raise_for_status()
            
            # XML íŒŒì‹± (ê°„ë‹¨ ë²„ì „)
            return self._parse_xml_response(response.text, pmids)
            
        except Exception as e:
            print(f"ë…¼ë¬¸ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_xml_response(self, xml_text: str, pmids: list[str]) -> list[PubMedPaper]:
        """XML ì‘ë‹µ íŒŒì‹± (ê°„ë‹¨ êµ¬í˜„)"""
        import re
        
        papers = []
        
        for pmid in pmids:
            try:
                # ì œëª© ì¶”ì¶œ
                title_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<ArticleTitle>(.+?)</ArticleTitle>',
                    xml_text, re.DOTALL
                )
                title = title_match.group(1) if title_match else f"Paper {pmid}"
                title = re.sub(r'<[^>]+>', '', title)  # HTML íƒœê·¸ ì œê±°
                
                # ì €ë„ ì¶”ì¶œ
                journal_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<Journal>.*?<Title>(.+?)</Title>',
                    xml_text, re.DOTALL
                )
                journal = journal_match.group(1) if journal_match else ""
                
                # ì—°ë„ ì¶”ì¶œ
                year_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<PubDate>.*?<Year>(\d{4})</Year>',
                    xml_text, re.DOTALL
                )
                pub_year = int(year_match.group(1)) if year_match else 2024
                
                # ì´ˆë¡ ì¶”ì¶œ
                abstract_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<Abstract>.*?<AbstractText[^>]*>(.+?)</AbstractText>',
                    xml_text, re.DOTALL
                )
                abstract = abstract_match.group(1) if abstract_match else ""
                abstract = re.sub(r'<[^>]+>', '', abstract)[:500]  # 500ì ì œí•œ
                
                papers.append(PubMedPaper(
                    pmid=pmid,
                    title=title[:200],
                    abstract=abstract,
                    journal=journal[:100],
                    pub_year=pub_year,
                    url=f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                ))
                
            except Exception as e:
                print(f"PMID {pmid} íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return papers
    
    def _get_cached_papers(self, query: str) -> Optional[list[PubMedPaper]]:
        """ìºì‹œëœ ë…¼ë¬¸ ì¡°íšŒ"""
        try:
            query_hash = hashlib.md5(query.encode()).hexdigest()
            
            result = self.db.table("pubmed_papers").select("*").filter(
                "pmid", "in", f"(SELECT pmid FROM pubmed_cache WHERE query_hash = '{query_hash}')"
            ).execute()
            
            # ê°„ë‹¨íˆ DBì—ì„œ ì§ì ‘ ì¡°íšŒ
            # ì‹¤ì œë¡œëŠ” ë³„ë„ ìºì‹œ í…Œì´ë¸” í•„ìš”
            return None
            
        except Exception:
            return None
    
    def _cache_papers(self, query: str, papers: list[PubMedPaper]):
        """ë…¼ë¬¸ ìºì‹œ ì €ì¥"""
        try:
            for paper in papers:
                self.db.table("pubmed_papers").upsert({
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "abstract": paper.abstract,
                    "journal": paper.journal,
                    "pub_year": paper.pub_year,
                    "url": paper.url,
                    "updated_at": datetime.now().isoformat()
                }, on_conflict="pmid").execute()
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")


def search_pubmed_papers(query: str, max_results: int = 2) -> list[dict]:
    """í¸ì˜ í•¨ìˆ˜: PubMed ë…¼ë¬¸ ê²€ìƒ‰"""
    service = PubMedService()
    papers = service.search_papers(query, max_results)
    
    return [
        {
            "pmid": p.pmid,
            "title": p.title,
            "abstract": p.abstract[:200] + "..." if len(p.abstract) > 200 else p.abstract,
            "journal": p.journal,
            "pub_year": p.pub_year,
            "url": p.url
        }
        for p in papers
    ]


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    queries = [
        "ginger digestive function",
        "jujube sleep improvement",
        "radish stomach health"
    ]
    
    for query in queries:
        print(f"\n{'='*50}")
        print(f"ê²€ìƒ‰: {query}")
        print("="*50)
        
        papers = search_pubmed_papers(query)
        for p in papers:
            print(f"ğŸ“„ {p['title'][:60]}...")
            print(f"   {p['journal']} ({p['pub_year']})")
            print(f"   {p['url']}")
