"""
PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
E-utilities APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ê²€ìƒ‰ ë° ìºì‹±
"""
import os
import sys
import hashlib
import requests
from typing import Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
from dotenv import load_dotenv

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from database.supabase_client import get_supabase_client

load_dotenv()


@dataclass
class PubMedPaper:
    """PubMed ë…¼ë¬¸ ì •ë³´"""
    pmid: str
    title: str
    abstract: str
    journal: str
    pub_year: int
    url: str
    summary_ko: Optional[str] = None


class PubMedService:
    """PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    
    def __init__(self):
        self.api_key = os.getenv("PUBMED_API_KEY", "")
        self.genai_key = os.getenv("API_KEY", "")
        self.db = get_supabase_client()
        
        # Configure GenAI for translation
        if self.genai_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.genai_key)
                self.model = genai.GenerativeModel('gemini-2.0-flash')
            except Exception as e:
                print(f"GenAI Init Failed: {e}")
                self.model = None
        else:
            self.model = None

    async def translate_to_english(self, keyword_ko: str) -> str:
        """í•œê¸€ ì˜í•™/ì‹ì¬ë£Œ í‚¤ì›Œë“œë¥¼ ì˜ë¬¸(MeSH Term)ìœ¼ë¡œ ë³€í™˜"""
        # 0. Try Google Translation API (REST) - User Suggestion
        if self.genai_key:
            try:
                url = "https://translation.googleapis.com/language/translate/v2"
                params = {
                    "q": keyword_ko,
                    "source": "ko",
                    "target": "en",
                    "format": "text",
                    "key": self.genai_key
                }
                # Using requests (sync) for simplicity, matching existing style
                resp = requests.post(url, params=params, timeout=5)
                if resp.status_code == 200:
                    data = resp.json()
                    if "data" in data and "translations" in data["data"]:
                        return data["data"]["translations"][0]["translatedText"]
                else:
                     print(f"Translation API Error: {resp.status_code} {resp.text}")
            except Exception as e:
                print(f"Google Translate API Failed: {e}")

        # 1. Try Gemini if available
        if self.model:
            try:
                prompt = f"""
                Translate the following Korean medical/food term into its most appropriate English scientific or MeSH term for PubMed search.
                Only return the English term. No explanation.
                
                Korean: {keyword_ko}
                English:
                """
                response = await self.model.generate_content_async(prompt)
                return response.text.strip()
            except Exception as e:
                print(f"Gemini Translation error: {e}")
                # Fallthrough to OpenAI
        
        # 2. Try OpenAI Fallback
        try:
            import openai
            openai_key = os.getenv("OPENAI_API_KEY")
            if openai_key:
                client = openai.AsyncOpenAI(api_key=openai_key)
                response = await client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "Translate the Korean medical/food term to English MeSH term. Return only English text."},
                        {"role": "user", "content": keyword_ko}
                    ]
                )
                return response.choices[0].message.content.strip()
        except Exception as e_openai:
            print(f"OpenAI Translation Failed: {e_openai}")

        return keyword_ko

    
    async def search_papers(
        self, 
        query: str, 
        max_results: int = 3,
        use_cache: bool = True
    ) -> list[PubMedPaper]:
        """
        PubMedì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ë¬¸)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            list[PubMedPaper]: ë…¼ë¬¸ ëª©ë¡
        """
        if any(ord(char) > 127 for char in query):
            # í•œê¸€ í¬í•¨ ì‹œ ìë™ ë²ˆì—­
            translated_query = await self.translate_to_english(query)
            print(f"Translated '{query}' -> '{translated_query}'")
            query = translated_query

        # ìºì‹œ í™•ì¸
        if use_cache:
            cached = self._get_cached_papers(query)
            if cached:
                return cached[:max_results]
        
        # PubMed ê²€ìƒ‰
        pmids = self._search_pmids(query, max_results)
        if not pmids:
            return []
        
        # ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        papers = self._fetch_paper_details(pmids)
        
        # ìºì‹œ ì €ì¥
        if papers:
            self._cache_papers(query, papers)
        
        return papers
    
    async def search_by_symptom_and_ingredient(
        self, 
        symptom_id: int, 
        rep_code: str
    ) -> list[PubMedPaper]:
        """
        ì¦ìƒ-ì‹ì¬ë£Œ ì¡°í•©ìœ¼ë¡œ ë…¼ë¬¸ ê²€ìƒ‰
        symptom_pubmed_map, ingredient_pubmed_map í…Œì´ë¸” í™œìš©
        """
        try:
            # ì¦ìƒ í‚¤ì›Œë“œ ì¡°íšŒ
            symptom_result = self.db.table("symptom_pubmed_map").select(
                "keyword_en, mesh_term"
            ).eq("symptom_id", symptom_id).order("priority").limit(2).execute()
            
            # ì‹ì¬ë£Œ í‚¤ì›Œë“œ ì¡°íšŒ
            ingredient_result = self.db.table("ingredient_pubmed_map").select(
                "ingredient_name_en, mesh_term, bioactive_compound"
            ).eq("rep_code", rep_code).order("priority").limit(2).execute()
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            symptom_terms = []
            for row in symptom_result.data:
                if row.get("mesh_term"):
                    symptom_terms.append(f'"{row["mesh_term"]}"[MeSH]')
                elif row.get("keyword_en"):
                    symptom_terms.append(row["keyword_en"])
            
            ingredient_terms = []
            for row in ingredient_result.data:
                if row.get("mesh_term"):
                    ingredient_terms.append(f'"{row["mesh_term"]}"[MeSH]')
                elif row.get("ingredient_name_en"):
                    ingredient_terms.append(row["ingredient_name_en"])
                if row.get("bioactive_compound"):
                    ingredient_terms.append(row["bioactive_compound"])
            
            if not symptom_terms or not ingredient_terms:
                return []
            
            # ANDë¡œ ì¡°í•©
            query = f"({' OR '.join(symptom_terms)}) AND ({' OR '.join(ingredient_terms)})"
            
            return await self.search_papers(query, max_results=2)
            
        except Exception as e:
            print(f"ì¦ìƒ-ì‹ì¬ë£Œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_pmids(self, query: str, max_results: int) -> list[str]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ PMID ëª©ë¡ ì¡°íšŒ"""
        try:
            params = {
                "db": "pubmed",
                "term": query,
                "retmax": max_results,
                "retmode": "json",
                "sort": "relevance"
            }
            if self.api_key:
                params["api_key"] = self.api_key
            
            response = requests.get(
                f"{self.BASE_URL}/esearch.fcgi",
                params=params,
                timeout=10
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("esearchresult", {}).get("idlist", [])
            
        except Exception as e:
            print(f"PMID ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _fetch_paper_details(self, pmids: list[str]) -> list[PubMedPaper]:
        """PMIDë¡œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        try:
            params = {
                "db": "pubmed",
                "id": ",".join(pmids),
                "retmode": "xml"
            }
            if self.api_key:
                params["api_key"] = self.api_key
            
            response = requests.get(
                f"{self.BASE_URL}/efetch.fcgi",
                params=params,
                timeout=15
            )
            response.raise_for_status()
            
            # XML íŒŒì‹± (ê°„ë‹¨ ë²„ì „)
            return self._parse_xml_response(response.text, pmids)
            
        except Exception as e:
            print(f"ë…¼ë¬¸ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_xml_response(self, xml_text: str, pmids: list[str]) -> list[PubMedPaper]:
        """XML ì‘ë‹µ íŒŒì‹± (ê°„ë‹¨ êµ¬í˜„)"""
        import re
        
        papers = []
        
        for pmid in pmids:
            try:
                # ì œëª© ì¶”ì¶œ
                title_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<ArticleTitle>(.+?)</ArticleTitle>',
                    xml_text, re.DOTALL
                )
                title = title_match.group(1) if title_match else f"Paper {pmid}"
                title = re.sub(r'<[^>]+>', '', title)  # HTML íƒœê·¸ ì œê±°
                
                # ì €ë„ ì¶”ì¶œ
                journal_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<Journal>.*?<Title>(.+?)</Title>',
                    xml_text, re.DOTALL
                )
                journal = journal_match.group(1) if journal_match else ""
                
                # ì—°ë„ ì¶”ì¶œ
                year_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<PubDate>.*?<Year>(\d{4})</Year>',
                    xml_text, re.DOTALL
                )
                pub_year = int(year_match.group(1)) if year_match else 2024
                
                # ì´ˆë¡ ì¶”ì¶œ
                abstract_match = re.search(
                    r'<PMID[^>]*>' + pmid + r'</PMID>.*?<Abstract>.*?<AbstractText[^>]*>(.+?)</AbstractText>',
                    xml_text, re.DOTALL
                )
                abstract = abstract_match.group(1) if abstract_match else ""
                abstract = re.sub(r'<[^>]+>', '', abstract)[:500]  # 500ì ì œí•œ
                
                papers.append(PubMedPaper(
                    pmid=pmid,
                    title=title[:200],
                    abstract=abstract,
                    journal=journal[:100],
                    pub_year=pub_year,
                    url=f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                ))
                
            except Exception as e:
                print(f"PMID {pmid} íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return papers
    
    def _get_cached_papers(self, query: str) -> Optional[list[PubMedPaper]]:
        """ìºì‹œëœ ë…¼ë¬¸ ì¡°íšŒ"""
        try:
            query_hash = hashlib.md5(query.encode()).hexdigest()
            
            result = self.db.table("pubmed_papers").select("*").filter(
                "pmid", "in", f"(SELECT pmid FROM pubmed_cache WHERE query_hash = '{query_hash}')"
            ).execute()
            
            # ê°„ë‹¨íˆ DBì—ì„œ ì§ì ‘ ì¡°íšŒ
            # ì‹¤ì œë¡œëŠ” ë³„ë„ ìºì‹œ í…Œì´ë¸” í•„ìš”
            return None
            
        except Exception:
            return None
    
    def _cache_papers(self, query: str, papers: list[PubMedPaper]):
        """ë…¼ë¬¸ ìºì‹œ ì €ì¥"""
        try:
            for paper in papers:
                self.db.table("pubmed_papers").upsert({
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "abstract": paper.abstract,
                    "journal": paper.journal,
                    "pub_year": paper.pub_year,
                    "url": paper.url,
                    "updated_at": datetime.now().isoformat()
                }, on_conflict="pmid").execute()
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")


async def search_pubmed_papers(query: str, max_results: int = 2) -> list[dict]:
    """í¸ì˜ í•¨ìˆ˜: PubMed ë…¼ë¬¸ ê²€ìƒ‰"""
    service = PubMedService()
    papers = await service.search_papers(query, max_results)
    
    return [
        {
            "pmid": p.pmid,
            "title": p.title,
            "abstract": p.abstract[:200] + "..." if len(p.abstract) > 200 else p.abstract,
            "journal": p.journal,
            "pub_year": p.pub_year,
            "url": p.url
        }
        for p in papers
    ]


if __name__ == "__main__":
    import asyncio
    
    async def main():
        # í…ŒìŠ¤íŠ¸
        queries = [
            "ginger digestive function",
            "ë„ë¼ì§€ ê¸°ì¹¨",  # í•œê¸€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            "radish stomach health"
        ]
        
        for query in queries:
            print(f"\n{'='*50}")
            print(f"ê²€ìƒ‰: {query}")
            print("="*50)
            
            papers = await search_pubmed_papers(query)
            for p in papers:
                print(f"ğŸ“„ {p['title'][:60]}...")
                print(f"   {p['journal']} ({p['pub_year']})")
                print(f"   {p['url']}")

    asyncio.run(main())
